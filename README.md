# ECE209AS
This repo contains project details for ECE 209AS Lip Reading Project. This project is done under Prof. Mani Srivastava during the Spring Quarter in UCLA (academic year: 2021 - 2022).

# Team Members
1. Shweta Katti (UID: 505604846)
2. Amulya Shruthi Tammireddi (UID: 505626283) - ashruthi1797@g.ucla.edu

# Project Description 
The project focuses on lip reading and maximizing efficiency of detection on existing video datasets. The problem statement can be found on 

# Code Base
All code changes can be found along the path https://github.com/amulyashruthi1797/ECE209AS/

# Literature Survey and Previous Work 
| Paper citation                                             | Methodology                                                                                      | 
| ---------------------------------------------------------- | -------------------------------------------------------------------------------------------- | 
|Chung, Joon Son, et al. "Lip reading sentences in the wild." 2017 IEEE conference on computer vision and pattern recognition (CVPR). IEEE, 2017.|‘Watch, Listen, Attend
and Spell’ (WLAS) network :The convolutional network is based on the VGG-M
model, followed by LSTM encoder and transducer.| 
|Assael, Yannis M., et al. "Lipnet: End-to-end sentence-level lipreading." arXiv preprint arXiv:1611.01599 (2016).|LipNet - CNN+BiGRU+CTC loss |
|Afouras, Triantafyllos, Joon Son Chung, and Andrew Zisserman. "The conversation: Deep audio-visual speech enhancement." arXiv preprint arXiv:1804.04121 (2018).| The network consists of a 3D convolution layer, followed by a 18-layer ResNet. |

# Work Done
1. Literature survey was completed for around 25 papers. Primary architectures that can be chosen for the detection were shortlisted. Also publicly available datasets 
 for the same were chosen.
2.

# Upcoming Targets
1.
